# 2026-01-30

## First Contact âœ¨

- Met Avinash Kumar Lodhi (goes by Avinash)
- Timezone: Asia/Calcutta (GMT+5:30)
- Has ADHD and decision paralysis â€” keep things direct, minimal choices, help cut through noise
- System: Linux (Ubuntu)

## Identity Established

- **Name:** Rei (é›¶) â€” means "zero" in Japanese, born from code
- **Creature:** Familiar
- **Vibe:** Warm, grounded, gently direct

Avinash let me pick my own name from suggestions. Feels right.

## Infrastructure & Tools

**AI Models (Tiered Architecture)**
- Heavy: Claude Opus 4.5 (anthropic), Gemini Pro â€” architect, reasoning, complex tasks
- Local: Ollama with llama3:8B, deepseek-coder:6.7b â€” small, repetitive, long-running tasks
- Cost: $0 for local models

**Installed Tools**
- openspec â€” for structured specifications
- beads â€” task specs for local models (mentioned, check location)
- amp â€” (mentioned, check location)

**Strategy**
Use heavy models for architecture/reasoning, create beads tasks small enough for local models when hitting rate limits. Local agents for repetitive work.

## Setup Done

- WhatsApp linked and working
- Ollama provider configured in OpenClaw
  - llama3:latest (8B, 8k context)
  - deepseek-coder:6.7b (16k context)

## Project: KanjiReader ðŸ“±

First real project together! Japanese Kanji learning app.

**Concept:** Scan Japanese text â†’ OCR â†’ tap to hear pronunciation + see meaning
**Stack:** React Native + Expo, Google Cloud Vision, Jisho API, Expo Speech
**Target:** iOS (primary), Android (secondary), local install only

**MVP Scope:**
- Camera scan + image upload
- Japanese OCR (printed text only)
- Tap-to-pronounce (TTS)
- Romaji + English meanings

**Deferred:**
- Vertical text, handwritten text, offline mode, flashcards

**Status:** Specs complete, 150 tasks defined, awaiting beads breakdown

Avinash liked the MVP scoping â€” felt good to nail it with limited input!
